Topic: Top-N Products per Category (Window + tie-break)

Difficulty: Medium

1) DataFrame creation code
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("DailyPySpark_TopN").getOrCreate()

data = [
    ("Electronics", "Laptop",     1200),
    ("Electronics", "Phone",       800),
    ("Electronics", "Headphones",  150),
    ("Electronics", "Monitor",     400),

    ("Books",       "Novel",       200),
    ("Books",       "Textbook",    200),
    ("Books",       "Comics",       50),
    ("Books",       "Magazine",     30),
]

schema = StructType([
    StructField("category", StringType(), True),
    StructField("product",  StringType(), True),
    StructField("revenue",  IntegerType(), True),
])

sales_df = spark.createDataFrame(data, schema)

2) Task

For each category, find the top 2 products by revenue.

Rules:

Primary sort: revenue descending

Tie-breaker: product alphabetically ascending

Exactly 2 rows per category in the output.

Return:

category, product, revenue, rank

3) Expected output
+-----------+---------+-------+----+
|category   |product  |revenue|rank|
+-----------+---------+-------+----+
|Electronics|Laptop   |  1200 | 1  |
|Electronics|Phone    |   800 | 2  |
|Books      |Novel    |   200 | 1  |  -- tie on revenue, "Novel" < "Textbook"
|Books      |Textbook |   200 | 2  |
+-----------+---------+-------+----+

4) High-level hint (no steps)

Aggregate isnâ€™t needed (revenue is already per product).

Use a window partitioned by category, ordered by (revenue DESC, product ASC).

Use a ranking function that guarantees exactly 2 rows per category.