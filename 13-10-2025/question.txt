Hereâ€™s a nice **ğŸª¶ light PySpark daily practice question** for your â€œtiredâ€ day ğŸ˜„

---

### ğŸ§  **PySpark Daily Question â€” Filtering & Simple Transformations**

You have a DataFrame `products`:

| product_id | name    | category    | price |
| ---------- | ------- | ----------- | ----- |
| 1          | Laptop  | Electronics | 800   |
| 2          | Mouse   | Electronics | 20    |
| 3          | T-Shirt | Clothing    | 15    |
| 4          | Shoes   | Footwear    | 60    |
| 5          | Phone   | Electronics | 600   |
| 6          | Jacket  | Clothing    | 120   |

---

### ğŸ§° **Task**

Using PySpark:

1. Filter all products in the **â€œElectronicsâ€** category.
2. Further filter only those with a **price greater than 100**.
3. Select only `name` and `price` columns and display the result.

ğŸ‘‰ Bonus (optional): Sort the result in descending order of price.

---

### ğŸ§‘â€ğŸ’» **Code to Create the DataFrame**

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Initialize Spark
spark = SparkSession.builder.appName("DailyPySparkPractice").getOrCreate()

# Sample data
products_data = [
    (1, "Laptop", "Electronics", 800),
    (2, "Mouse", "Electronics", 20),
    (3, "T-Shirt", "Clothing", 15),
    (4, "Shoes", "Footwear", 60),
    (5, "Phone", "Electronics", 600),
    (6, "Jacket", "Clothing", 120)
]

# Schema
products_schema = StructType([
    StructField("product_id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("category", StringType(), True),
    StructField("price", IntegerType(), True)
])

# Create DataFrame
products_df = spark.createDataFrame(products_data, schema=products_schema)

# Show DataFrame
products_df.show()
```

---

### ğŸ **Expected Output (After Filtering)**

| name   | price |
| ------ | ----- |
| Laptop | 800   |
| Phone  | 600   |

Nice and easy for today ğŸ˜
Would you like me to give tomorrowâ€™s one a bit harder (e.g., involving `groupBy`)?
