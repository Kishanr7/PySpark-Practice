Got it ðŸ‘ Iâ€™ll now only send the 7 PM IST PySpark question if you havenâ€™t already completed one that day.

Hereâ€™s **todayâ€™s** PySpark question (kept focused and light ðŸ‘‡).

---

## ðŸ§© **Problem â€” Average Order Value per Customer**

You have an e-commerce dataset with customer orders.

### ðŸŽ¯ **Tasks**

1. For each customer (`cust_id`), calculate:

   * Total number of orders
   * Total quantity purchased
   * Total amount spent (`quantity * unit_price`)
   * Average order value (`total_amount / total_orders`)

2. Identify customers whose **average order value > 500**.

3. Sort the results by `avg_order_value` (descending).

---

### ðŸ’» **DataFrame Creation Code**

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

spark = SparkSession.builder.appName("DailyPySparkQuestion").getOrCreate()

data = [
    ("O1", "C1", "2025-10-01", "Laptop", 1, 1200.0),
    ("O2", "C1", "2025-10-05", "Mouse", 2, 25.0),
    ("O3", "C2", "2025-10-03", "Phone", 1, 800.0),
    ("O4", "C2", "2025-10-04", "Headset", 2, 100.0),
    ("O5", "C3", "2025-10-06", "Keyboard", 1, 90.0),
    ("O6", "C3", "2025-10-07", "Mouse", 1, 30.0),
    ("O7", "C4", "2025-10-02", "Laptop", 1, 1100.0),
    ("O8", "C4", "2025-10-08", "Monitor", 1, 300.0),
]

schema = StructType([
    StructField("order_id", StringType(), True),
    StructField("cust_id", StringType(), True),
    StructField("order_date", StringType(), True),
    StructField("product", StringType(), True),
    StructField("quantity", IntegerType(), True),
    StructField("unit_price", DoubleType(), True),
])

orders_df = spark.createDataFrame(data, schema=schema)
orders_df.show()
```

---

Keep it simple:

* Use `groupBy("cust_id")` and `agg()` for totals.
* Add a computed column for `avg_order_value`.
* Apply the filter and order the final result.
