ðŸ“… Problem Date

29 Jan 2026

ðŸ§© Problem Name

Sessionization with Inactivity Threshold

ðŸ”¥ Difficulty

Mediumâ€“Hard

ðŸ“˜ Problem Statement

You are given event-level user activity data with timestamps.
A new session starts for a user when the time gap between two consecutive events exceeds 30 minutes.

Your task is to assign a session_id to each event such that:

Events are grouped by user_id

Ordered by event_time

A new session starts if the gap from the previous event is > 30 minutes

Session IDs should be sequential per user, starting from 1

ðŸ§ª Input DataFrame (Creation Code)
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.getOrCreate()

data = [
    ("U1", "2026-01-29 09:00:00"),
    ("U1", "2026-01-29 09:10:00"),
    ("U1", "2026-01-29 10:00:00"),
    ("U1", "2026-01-29 10:20:00"),
    ("U1", "2026-01-29 11:00:00"),

    ("U2", "2026-01-29 08:00:00"),
    ("U2", "2026-01-29 08:20:00"),
    ("U2", "2026-01-29 09:00:00"),
]

df = (
    spark.createDataFrame(data, ["user_id", "event_time"])
    .withColumn("event_time", F.to_timestamp("event_time"))
)

ðŸ“¤ Expected Output
+-------+-------------------+-----------+
|user_id|event_time         |session_id |
+-------+-------------------+-----------+
|U1     |2026-01-29 09:00:00|1          |
|U1     |2026-01-29 09:10:00|1          |
|U1     |2026-01-29 10:00:00|2          |
|U1     |2026-01-29 10:20:00|2          |
|U1     |2026-01-29 11:00:00|3          |
|U2     |2026-01-29 08:00:00|1          |
|U2     |2026-01-29 08:20:00|1          |
|U2     |2026-01-29 09:00:00|2          |

ðŸ§  High-Level Hints (minimal)

Requires ordering + state awareness

Think in terms of time differences between rows

This is a classic window + cumulative logic problem

Avoid UDFs

This is a real-world analytics pattern that comes up constantly.
Clean logic and correct windowing matter more than clever tricks.