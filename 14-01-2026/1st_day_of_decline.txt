### üß† Backlog Catch-Up ‚Äî **Jan 14 PySpark Question**

**Date:** **2026-01-14**
**Difficulty:** **Medium**

---

## üìå Problem: First Day of Decline per Product

You are given a table of **daily sales amounts** for products.

For **each product**, find the **first date** on which the **sales amount decreases compared to the previous day**.

Rules:

* Comparison is **within each product**, ordered by date
* If a product **never declines**, return `NULL`
* Output must contain **exactly one row per product**

---

### 1Ô∏è‚É£ DataFrame Creation Code

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.getOrCreate()

data = [
    ("P1", "2026-01-01", 100),
    ("P1", "2026-01-02", 120),
    ("P1", "2026-01-03", 110),
    ("P1", "2026-01-04", 130),
    ("P2", "2026-01-01", 200),
    ("P2", "2026-01-02", 210),
    ("P2", "2026-01-03", 220),
    ("P3", "2026-01-01", 50),
]

sales_df = (
    spark.createDataFrame(data, ["product_id", "sale_date", "amount"])
         .withColumn("sale_date", F.to_date("sale_date"))
)

sales_df.show(truncate=False)
```

---

### 2Ô∏è‚É£ Expected Output

```
+----------+-----------+
|product_id|sale_date  |
+----------+-----------+
|P1        |2026-01-03 |
|P2        |NULL       |
|P3        |NULL       |
+----------+-----------+
```

---

### 3Ô∏è‚É£ Minimal Hints (High-Level Only)

* You‚Äôll need **ordering within partitions**
* Compare each row with its **previous row**
* Be careful not to drop products that never decline
* ‚ÄúFirst‚Äù means **earliest by date**, not smallest value

---

Post your solution when ready, and I‚Äôll review it.
Once this is done, we‚Äôll move to **Jan 15 ‚Äì Medium-Hard**.
