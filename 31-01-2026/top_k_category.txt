ðŸ§© Todayâ€™s PySpark Interview Question
ðŸ“… Problem Date

31 Jan 2026

ðŸ”¥ Difficulty

Hard

ðŸ“˜ Problem Name

Topâ€‘K Categories per User with Stable Ties

ðŸ“˜ Problem Statement

You are given user transaction data with categories and amounts.

Your task is to compute, for each user, the top 2 categories ranked by:

Total spend (descending)

If totals tie, earliest transaction date wins

If still tied, order categories alphabetically

Additional rules:

A user may have fewer than 2 categories

Output must contain one row per (user, rank)

Ranking restarts per user

ðŸ§ª Input DataFrame (Creation Code)
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.getOrCreate()

data = [
    ("U1", "Books",  100, "2026-01-01"),
    ("U1", "Books",   50, "2026-01-03"),
    ("U1", "Games",  150, "2026-01-02"),
    ("U1", "Music",  150, "2026-01-01"),

    ("U2", "Books",   40, "2026-01-05"),
    ("U2", "Games",   40, "2026-01-01"),
    ("U2", "Movies",  20, "2026-01-03"),
]

df = (
    spark.createDataFrame(
        data,
        ["user_id", "category", "amount", "txn_date"]
    )
    .withColumn("txn_date", F.to_date("txn_date"))
)

ðŸ“¤ Expected Output
+-------+--------+-----+
|user_id|category|rank |
+-------+--------+-----+
|U1     |Music   |1    |
|U1     |Games   |2    |
|U2     |Games   |1    |
|U2     |Books   |2    |

ðŸ§  Highâ€‘Level Hints (minimal)

Requires aggregation before ranking

Tieâ€‘breaking order matters

Use window functions, not group-level filtering

Avoid hardcoding limit logic

This problem checks whether you can combine aggregation, deterministic ranking, and partitioned logic cleanly â€” a common failure point in real interviews.